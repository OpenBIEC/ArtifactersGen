{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184fa2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Using device: xpu\n"
     ]
    }
   ],
   "source": [
    "import os, math, glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# device 选择：CUDA > XPU > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch, \"xpu\") and torch.xpu.is_available():   # type: ignore\n",
    "    device = torch.device(\"xpu\")                           # type: ignore\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\">>> Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba61e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU(nn.Module):\n",
    "    def forward(self, x): return F.silu(x)\n",
    "\n",
    "def timestep_embedding(timesteps, dim):\n",
    "    \"\"\"\n",
    "    返回 (B, dim) 的 sin/cos 时间步嵌入\n",
    "    \"\"\"\n",
    "    device = timesteps.device\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(10000) * torch.arange(0, half, dtype=torch.float32, device=device) / half\n",
    "    )\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    if dim % 2:  # 维度为奇数时补 0\n",
    "        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
    "    return emb\n",
    "\n",
    "class FiLM(nn.Module):\n",
    "    \"\"\"把 time‑embedding 映射到 (scale, shift) 并加到特征图\"\"\"\n",
    "    def __init__(self, in_channels, time_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_dim, 2 * in_channels))\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        gamma, beta = self.mlp(t_emb).chunk(2, dim=-1)\n",
    "        gamma, beta = gamma[:, :, None, None], beta[:, :, None, None]\n",
    "        return x * (1 + gamma) + beta\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(32, out_c)\n",
    "        self.norm2 = nn.GroupNorm(32, out_c)\n",
    "        self.act   = SiLU()\n",
    "        self.film  = FiLM(out_c, time_dim)\n",
    "        self.skip  = nn.Conv2d(in_c, out_c, 1) if in_c != out_c else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.act(self.norm1(self.conv1(x)))\n",
    "        h = self.film(h, t_emb)\n",
    "        h = self.act(self.norm2(self.conv2(h)))\n",
    "        return h + self.skip(x)\n",
    "\n",
    "def center_crop_to(src, ref):\n",
    "    \"\"\"\n",
    "    将 src 在 H,W 维度上中心裁剪 / 适量 pad 到 ref 的空间尺寸\n",
    "    只会裁剪或对齐到更小边界，不改变 Batch/Channel 维\n",
    "    \"\"\"\n",
    "    _, _, Hs, Ws = src.shape\n",
    "    _, _, Hr, Wr = ref.shape\n",
    "\n",
    "    # center crop\n",
    "    if Hs > Hr:\n",
    "        dh = (Hs - Hr) // 2\n",
    "        src = src[:, :, dh:dh+Hr, :]\n",
    "    if Ws > Wr:\n",
    "        dw = (Ws - Wr) // 2\n",
    "        src = src[:, :, :, dw:dw+Wr]\n",
    "\n",
    "    # (可选) pad 到目标尺寸 —— 理论上 Hs>=Hr & Ws>=Wr 已满足\n",
    "    if src.shape[2] < Hr or src.shape[3] < Wr:\n",
    "        pad_h = Hr - src.shape[2]\n",
    "        pad_w = Wr - src.shape[3]\n",
    "        src = F.pad(src, (0, pad_w, 0, pad_h))\n",
    "\n",
    "    return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e847e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtifactMiniEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    仅输出两层特征：\n",
    "      A5  (512, 22, 22)\n",
    "      ABN (1024, 11, 11)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, 2, 3), SiLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1), SiLU(),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1), SiLU(),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1), SiLU()\n",
    "        )\n",
    "        self.down1024 = nn.Conv2d(512, 1024, 3, 2, 1)\n",
    "        self.act = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)                     # -> (*,512, H/16, W/16)\n",
    "        A5 = F.adaptive_avg_pool2d(h, (22, 22)) # fixed grid\n",
    "        h  = self.act(self.down1024(h))         # 1024\n",
    "        ABN = F.adaptive_avg_pool2d(h, (11, 11))\n",
    "        return A5, ABN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b967be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.to_q = nn.Linear(dim, dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim, dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        B, N, C = q.shape\n",
    "        h = self.heads\n",
    "        q = self.to_q(q).view(B, N, h, C // h).transpose(1, 2) * self.scale\n",
    "        k, v = self.to_kv(kv).chunk(2, dim=-1)\n",
    "        k = k.view(B, -1, h, C // h).transpose(1, 2)\n",
    "        v = v.view(B, -1, h, C // h).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)).softmax(dim=-1)\n",
    "        out = (att @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beae4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionUNet(nn.Module):\n",
    "    def __init__(self, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim)\n",
    "        )\n",
    "\n",
    "        # =========== Encoder ===========\n",
    "        self.res0 = ResBlock(3, 64, time_dim)\n",
    "        self.res0b = ResBlock(64, 64, time_dim)\n",
    "        self.down1 = nn.Conv2d(64, 128, 4, 2, 1)\n",
    "        self.res1a = ResBlock(128, 128, time_dim)\n",
    "        self.res1b = ResBlock(128, 128, time_dim)\n",
    "        self.down2 = nn.Conv2d(128, 256, 4, 2, 1)\n",
    "        self.res2a = ResBlock(256, 256, time_dim)\n",
    "        self.res2b = ResBlock(256, 256, time_dim)\n",
    "        self.down3 = nn.Conv2d(256, 512, 4, 2, 1)\n",
    "        self.res3a = ResBlock(512, 512, time_dim)\n",
    "        self.res3b = ResBlock(512, 512, time_dim)\n",
    "        self.down4 = nn.Conv2d(512, 512, 4, 2, 1)\n",
    "        self.res4a = ResBlock(512, 512, time_dim)\n",
    "        self.res4b = ResBlock(512, 512, time_dim)\n",
    "        self.down5 = nn.Conv2d(512, 512, 4, 2, 1)\n",
    "        self.res5a = ResBlock(512, 512, time_dim)\n",
    "        self.res5b = ResBlock(512, 512, time_dim)\n",
    "\n",
    "        # Artifact attention @ 22×22\n",
    "        self.att5 = CrossAttention(512)\n",
    "\n",
    "        # bottleneck 11×11\n",
    "        self.down_bn = nn.Conv2d(512, 1024, 4, 2, 1)\n",
    "        self.res_bn1 = ResBlock(1024, 1024, time_dim)\n",
    "\n",
    "        self.att_bn = CrossAttention(1024)\n",
    "        self.res_bn2 = ResBlock(1024, 1024, time_dim)\n",
    "\n",
    "        # =========== Decoder ===========\n",
    "        self.up5 = nn.ConvTranspose2d(1024, 512, 4, 2, 1)\n",
    "        self.up5_res1 = ResBlock(1024, 512, time_dim)   # 512+512\n",
    "        self.up5_res2 = ResBlock(512, 512, time_dim)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(512, 512, 4, 2, 1)\n",
    "        self.up4_res1 = ResBlock(1024, 512, time_dim)   # 512+512\n",
    "        self.up4_res2 = ResBlock(512, 512, time_dim)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
    "        self.up3_res1 = ResBlock(768, 256, time_dim)    # 256+512  ★\n",
    "        self.up3_res2 = ResBlock(256, 256, time_dim)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
    "        self.up2_res1 = ResBlock(384, 128, time_dim)    # 128+256  ★\n",
    "        self.up2_res2 = ResBlock(128, 128, time_dim)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.up1_res1 = ResBlock(192, 64, time_dim)     # 64+128   ★\n",
    "        self.up1_res2 = ResBlock(64, 64, time_dim)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(64, 3, 1)\n",
    "\n",
    "        # Artifact encoder\n",
    "        self.art_enc = ArtifactMiniEncoder()\n",
    "\n",
    "    # --------------------------\n",
    "    def forward(self, x, art, t):\n",
    "        t_emb = self.time_mlp(timestep_embedding(t, self.time_mlp[0].in_features))\n",
    "\n",
    "        # Encoder\n",
    "        h0 = self.res0b(self.res0(x, t_emb), t_emb)\n",
    "        h1 = self.res1b(self.res1a(self.down1(h0), t_emb), t_emb)\n",
    "        h2 = self.res2b(self.res2a(self.down2(h1), t_emb), t_emb)\n",
    "        h3 = self.res3b(self.res3a(self.down3(h2), t_emb), t_emb)\n",
    "        h4 = self.res4b(self.res4a(self.down4(h3), t_emb), t_emb)\n",
    "        h5 = self.res5b(self.res5a(self.down5(h4), t_emb), t_emb)   # 22×22\n",
    "\n",
    "        # Artifact features\n",
    "        A5, ABN = self.art_enc(art)\n",
    "        B, C, H, W = h5.shape\n",
    "        att_in = torch.cat([h5.flatten(2).transpose(1,2), A5.flatten(2).transpose(1,2)], dim=1)\n",
    "        h5 = self.att5(h5.flatten(2).transpose(1,2), att_in).transpose(1,2).view(B,C,H,W)\n",
    "\n",
    "        # Bottleneck 11×11\n",
    "        bn = self.res_bn1(self.down_bn(h5), t_emb)\n",
    "        B, Cb, Hb, Wb = bn.shape\n",
    "        att_in_bn = torch.cat([bn.flatten(2).transpose(1,2), ABN.flatten(2).transpose(1,2)], dim=1)\n",
    "        bn = self.att_bn(bn.flatten(2).transpose(1,2), att_in_bn).transpose(1,2).view(B,Cb,Hb,Wb)\n",
    "        bn = self.res_bn2(bn, t_emb)\n",
    "\n",
    "        # Decoder\n",
    "        up5_dec = self.up5(bn)\n",
    "        up5_dec = center_crop_to(up5_dec, h5)      # ★ new\n",
    "        up5 = torch.cat([up5_dec, h5], dim=1)\n",
    "        up5 = self.up5_res2(self.up5_res1(up5, t_emb), t_emb)\n",
    "\n",
    "        up4_dec = self.up4(up5)\n",
    "        up4_dec = center_crop_to(up4_dec, h4)      # ★ new\n",
    "        up4 = torch.cat([up4_dec, h4], dim=1)\n",
    "        up4 = self.up4_res2(self.up4_res1(up4, t_emb), t_emb)\n",
    "\n",
    "        up3_dec = self.up3(up4)\n",
    "        up3_dec = center_crop_to(up3_dec, h3)      # ★ new\n",
    "        up3 = torch.cat([up3_dec, h3], dim=1)\n",
    "        up3 = self.up3_res2(self.up3_res1(up3, t_emb), t_emb)\n",
    "\n",
    "        up2_dec = self.up2(up3)\n",
    "        up2_dec = center_crop_to(up2_dec, h2)      # ★ new\n",
    "        up2 = torch.cat([up2_dec, h2], dim=1)\n",
    "        up2 = self.up2_res2(self.up2_res1(up2, t_emb), t_emb)\n",
    "\n",
    "        up1_dec = self.up1(up2)\n",
    "        up1_dec = center_crop_to(up1_dec, h1)      # ★ new\n",
    "        up1 = torch.cat([up1_dec, h1], dim=1)\n",
    "        up1 = self.up1_res2(self.up1_res1(up1, t_emb), t_emb)\n",
    "\n",
    "        out = self.out_conv(up1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041d4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class ArtifactCharacterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_art='../Data/Source_Graph',\n",
    "                       root_role='../Data/Target_Graph'):\n",
    "        super().__init__()\n",
    "        self.art_files = sorted(glob.glob(os.path.join(root_art, '*.png')))\n",
    "        self.role_dir = root_role\n",
    "        self.trans_art = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.trans_role = transforms.Compose([\n",
    "            transforms.Resize((700,700)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self): return len(self.art_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        art_path = self.art_files[idx]\n",
    "        name = os.path.splitext(os.path.basename(art_path))[0]\n",
    "        role1 = Image.open(os.path.join(self.role_dir, f'{name} 肖形.png')).convert('RGB')\n",
    "        role2 = Image.open(os.path.join(self.role_dir, f'{name} 写照.png')).convert('RGB')\n",
    "        art_img = Image.open(art_path).convert('RGB')\n",
    "\n",
    "        # 这里仅返回 role1，可自行随机选择两张\n",
    "        return (self.trans_role(role1),\n",
    "                self.trans_art(art_img),\n",
    "                torch.randint(0, 1000, (1,)))  # dummy time‑step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac1a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ε‑pred output shape: torch.Size([1, 3, 350, 350])\n"
     ]
    }
   ],
   "source": [
    "model = DiffusionUNet().to(device)\n",
    "dummy_role = torch.randn(1,3,700,700, device=device)\n",
    "dummy_art  = torch.randn(1,3,800,512, device=device)   # 随意尺寸\n",
    "dummy_t    = torch.randint(0,1000,(1,), device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(dummy_role, dummy_art, dummy_t)\n",
    "print('ε‑pred output shape:', out.shape)\n",
    "\n",
    "# ---- torchinfo summary (可选) ----\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "    summary(model, input_data=(dummy_role, dummy_art, dummy_t), depth=2)\n",
    "except ImportError:\n",
    "    print(\"安装 torchinfo 后可获得层级参数统计： pip install torchinfo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e901b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======  1. 线性 beta 调度  =======\n",
    "T = 1000                                           # diffusion steps\n",
    "beta_start, beta_end = 1e-4, 2e-2\n",
    "betas  = torch.linspace(beta_start, beta_end, T, dtype=torch.float32, device=device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cum = torch.cumprod(alphas, dim=0)          # \\bar α_t\n",
    "\n",
    "# =======  2. q_sample  =======\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    \"\"\"\n",
    "    根据公式  x_t = sqrt(abar_t) x0 + sqrt(1-abar_t) ε\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    abar = alphas_cum[t].view(-1, 1, 1, 1)\n",
    "    return (abar.sqrt() * x_start + (1.0 - abar).sqrt() * noise)\n",
    "\n",
    "# =======  3. 随机时间步生成器  =======\n",
    "def sample_timesteps(batch_size):\n",
    "    return torch.randint(0, T, (batch_size,), device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d066d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = ArtifactCharacterDataset()\n",
    "loader  = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "model    = DiffusionUNet().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 可选：学习率调度\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100*len(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae4f7c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 532, 368] at entry 0 and [3, 460, 690] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      8\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrole_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mart_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrole_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrole_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mart_img\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mart_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TabYe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\TabYe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\TabYe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TabYe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TabYe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TabYe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\TabYe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 532, 368] at entry 0 and [3, 460, 690] at entry 1"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "save_every = 2        # 每 N epoch 保存一次\n",
    "ckpt_dir = \"./checkpoints\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for role_img, art_img, _ in loader:\n",
    "        role_img = role_img.to(device)\n",
    "        art_img  = art_img.to(device)\n",
    "        bsz = role_img.size(0)\n",
    "\n",
    "        # 1. 采样时间步 & 真实噪声\n",
    "        t     = sample_timesteps(bsz)\n",
    "        noise = torch.randn_like(role_img)\n",
    "\n",
    "        # 2. x_t 输入模型\n",
    "        x_t = q_sample(role_img, t, noise)\n",
    "        pred_noise = model(x_t, art_img, t)\n",
    "\n",
    "        # 3. 计算 MSE 损失\n",
    "        loss = criterion(pred_noise, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_sched.step()\n",
    "\n",
    "        running_loss += loss.item() * bsz\n",
    "\n",
    "    avg_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | loss={avg_loss:.6f}\")\n",
    "\n",
    "    # --------- checkpoint ----------\n",
    "    if epoch % save_every == 0:\n",
    "        ckpt_path = os.path.join(ckpt_dir, f\"epoch_{epoch:03d}.pt\")\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optim_state\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch\n",
    "        }, ckpt_path)\n",
    "        print(\"Checkpoint saved:\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fcd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
