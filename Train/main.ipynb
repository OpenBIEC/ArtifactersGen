{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b53a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "import os, random, math, json, glob, re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from einops import rearrange, repeat\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382bf183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文物缺失: ['可爱花束']\n",
      "写照缺失: []\n"
     ]
    }
   ],
   "source": [
    "missing_src, missing_other = [], []\n",
    "for role_path in Path(\"../Data/Target_Graph\").glob(\"* 肖形.png\"):\n",
    "    name = role_path.stem.split(\" \")[0]\n",
    "    src_path   = Path(\"../Data/Source_Graph\")/f\"{name}.png\"\n",
    "    other_path = Path(\"../Data/Target_Graph\")/f\"{name} 写照.png\"\n",
    "    if not src_path.exists():   missing_src.append(name)\n",
    "    if not other_path.exists(): missing_other.append(name)\n",
    "print(\"文物缺失:\", missing_src)\n",
    "print(\"写照缺失:\", missing_other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2df161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def pad_to_square_pil(im: Image.Image, fill=0):\n",
    "    w, h = im.size\n",
    "    l = max(w, h)\n",
    "    new = Image.new(\"RGB\", (l, l), (fill,)*3)\n",
    "    new.paste(im, ((l-w)//2, (l-h)//2))\n",
    "    return new\n",
    "\n",
    "def pad_square_1024(im: Image.Image):\n",
    "    if max(im.size) > 1024:\n",
    "        ratio = 1024 / max(im.size)\n",
    "        im = im.resize((round(im.size[0]*ratio), round(im.size[1]*ratio)),\n",
    "                       Image.BICUBIC)\n",
    "    w, h = im.size\n",
    "    new = Image.new(\"RGB\", (1024, 1024), (0,0,0))\n",
    "    new.paste(im, ((1024-w)//2, (1024-h)//2))\n",
    "    return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2df1399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class ArtifactAnimeDataset(Dataset):\n",
    "    def __init__(self, root_src=\"../Data/Source_Graph\",\n",
    "                 root_tgt=\"../Data/Target_Graph\"):\n",
    "        self.src_root = Path(root_src)\n",
    "        self.tgt_root = Path(root_tgt)\n",
    "\n",
    "        self.pairs = []\n",
    "        for role_path in self.tgt_root.glob(\"* 肖形.png\"):\n",
    "            name  = role_path.stem.split(\" \")[0]\n",
    "            src   = self.src_root/f\"{name}.png\"\n",
    "            other = self.tgt_root/f\"{name} 写照.png\"\n",
    "            if src.exists() and other.exists():\n",
    "                self.pairs.append((src, role_path, other))\n",
    "\n",
    "        self.tgt_tf = transforms.Compose([\n",
    "            transforms.Lambda(pad_to_square_pil),\n",
    "            transforms.Resize((700, 700), interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.src_tf = transforms.Compose([\n",
    "            transforms.Lambda(pad_square_1024),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_path, role_a, role_b = self.pairs[idx]\n",
    "        role_path = random.choice([role_a, role_b])\n",
    "        role_img  = Image.open(role_path).convert(\"RGB\")\n",
    "        src_img   = Image.open(src_path).convert(\"RGB\")\n",
    "        return self.src_tf(src_img), self.tgt_tf(role_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd0b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned samples: 99\n"
     ]
    }
   ],
   "source": [
    "dataset = ArtifactAnimeDataset()\n",
    "print(\"Aligned samples:\", len(dataset))\n",
    "loader  = DataLoader(dataset, batch_size=2, shuffle=True,\n",
    "                     num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69abbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaSchedule:\n",
    "    def __init__(self, n_steps=1000, max_beta=0.02, device=\"cpu\"):\n",
    "        self.N = n_steps\n",
    "        betas  = torch.linspace(1e-4, max_beta, n_steps)\n",
    "        alphas = 1. - betas\n",
    "        self.register_buffer = lambda n, x: setattr(self, n, x.to(device))\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_bar', torch.cumprod(alphas, 0))\n",
    "\n",
    "    def get_params(self, t):\n",
    "        return self.betas[t], self.alphas_bar[t]\n",
    "\n",
    "schedule = BetaSchedule(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab5304e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# --- 4.1 时间嵌入 ---\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim*4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim*4, time_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        half = self.time_dim // 2\n",
    "        # 位置编码\n",
    "        freqs = torch.exp(\n",
    "            torch.arange(half, device=t.device) * -(math.log(10000.0) / (half - 1))\n",
    "        )\n",
    "        args = t.float().unsqueeze(1) * freqs.unsqueeze(0)          # (B, half)\n",
    "        emb  = torch.cat([torch.sin(args), torch.cos(args)], dim=1) # (B, time_dim)\n",
    "        return self.mlp(emb)                                        # (B, time_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6366cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# --- 4.2 ResBlock with FiLM ---\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_dim=None, act='silu'):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, padding=1)\n",
    "        self.act   = getattr(F, act)\n",
    "        self.use_film = time_dim is not None and time_dim > 0\n",
    "        if self.use_film:\n",
    "            self.time_proj = nn.Linear(time_dim, out_c * 2)\n",
    "        else:\n",
    "            self.time_proj = None\n",
    "        self.skip = nn.Conv2d(in_c, out_c, 1) if in_c != out_c else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb=None):\n",
    "        h = self.act(self.conv1(x))\n",
    "        if self.use_film and t_emb is not None:\n",
    "            γ, β = self.time_proj(t_emb).chunk(2, dim=1)  # (B, C)\n",
    "            h = h * (1 + γ[..., None, None]) + β[..., None, None]\n",
    "        h = self.conv2(h)\n",
    "        return self.act(h + self.skip(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "595b32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# --- 4.3 文物 Adaptive FPN Encoder ---\n",
    "class CondEncoder(nn.Module):\n",
    "    def __init__(self, channels=(64,128,256,512,768)):\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList()\n",
    "        in_c=3\n",
    "        for c in channels:\n",
    "            self.stages.append(nn.Sequential(\n",
    "                ResBlock(in_c,c,0),       # no time emb\n",
    "                ResBlock(c,c,0),\n",
    "                nn.Conv2d(c,c,2,stride=2) # Down2\n",
    "            ))\n",
    "            in_c=c\n",
    "        self.target_sizes = [(350,350),(175,175),(88,88),(44,44)]\n",
    "        self.proj = nn.ModuleList([nn.Conv2d(c,c,1) for c in channels[:-1]])\n",
    "    def forward(self, x):\n",
    "        feats=[]\n",
    "        for i,stage in enumerate(self.stages):\n",
    "            x = stage(x)\n",
    "            if i<4:\n",
    "                pooled = F.adaptive_avg_pool2d(x, self.target_sizes[i])\n",
    "                feats.append(self.proj[i](pooled))\n",
    "        feats.append(x)  # 最深层原尺寸44x44\n",
    "        return feats   # len=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa4f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# --- 4.4 Role Encoder / Decoder with Skip-Attn 融合 ---\n",
    "class SkipAttnFusion(nn.Module):\n",
    "    def __init__(self, c, heads):\n",
    "        super().__init__()\n",
    "        self.q = nn.Conv2d(c, c, 1)\n",
    "        self.kv = nn.Conv2d(2*c, c*2,1)\n",
    "        self.fc = nn.Conv2d(c, c,1)\n",
    "        self.heads=heads; self.scale = (c//heads)**-0.5\n",
    "    def forward(self, dec, role, art):\n",
    "        B,C,H,W = dec.shape\n",
    "        q = self.q(dec).reshape(B,self.heads,C//self.heads,H*W)\n",
    "        kv = self.kv(torch.cat([role,art],1))\n",
    "        k,v = kv.chunk(2,1)\n",
    "        k = k.reshape(B,self.heads,C//self.heads,-1)\n",
    "        v = v.reshape(B,self.heads,C//self.heads,-1)\n",
    "        attn = (q.transpose(-2,-1) @ k)*self.scale\n",
    "        attn = attn.softmax(-1)\n",
    "        out  = (v @ attn.transpose(-2,-1)).reshape(B,C,H,W)\n",
    "        return dec + self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cceee0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_dim, is_last=False):\n",
    "        super().__init__()\n",
    "        self.res1 = ResBlock(in_c,  out_c, time_dim)\n",
    "        self.res2 = ResBlock(out_c, out_c, time_dim)\n",
    "        self.down = None\n",
    "        if not is_last:                      # bottleneck 不再下采样\n",
    "            self.down = nn.Conv2d(out_c, out_c, 2, stride=2)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        if self.down is not None:\n",
    "            x = self.down(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_dim):\n",
    "        super().__init__()\n",
    "        self.up   = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.res1 = ResBlock(in_c,  out_c, time_dim)\n",
    "        self.res2 = ResBlock(out_c, out_c, time_dim)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.up(x)\n",
    "        x = self.res1(x, t_emb)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "373842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "class UNetDiff(nn.Module):\n",
    "    def __init__(self, time_dim=256, base_c=(64,128,256,512,768), act='silu'):\n",
    "        super().__init__()\n",
    "        self.time_emb = TimeEmbedding(time_dim=256)\n",
    "        # Role encoder\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        in_c = 3\n",
    "        for i, c in enumerate(base_c):\n",
    "            is_last = (i == len(base_c)-1)\n",
    "            self.down_blocks.append(DownBlock(in_c, c, time_dim, is_last=is_last))\n",
    "            in_c = c\n",
    "        # Cond encoder\n",
    "        self.cond_enc = CondEncoder(base_c)\n",
    "        # Skip Attention\n",
    "        self.skip_fus = nn.ModuleList([SkipAttnFusion(c,max(4,c//64)) for c in base_c[::-1]])\n",
    "        # Up path\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        rev = base_c[::-1]\n",
    "        for idx in range(len(rev)-1):\n",
    "            self.up_blocks.append(UpBlock(rev[idx], rev[idx+1], time_dim))\n",
    "        # Bottleneck extra block\n",
    "        self.mid = ResBlock(base_c[-1],base_c[-1],time_dim,act)\n",
    "        # Output\n",
    "        self.out_head = nn.Sequential(nn.Conv2d(base_c[0],3,1))\n",
    "    def forward(self, role_noisy, artifact, t):\n",
    "        t_emb = self.time_emb(t)\n",
    "        role_feats = []\n",
    "        x = role_noisy\n",
    "        for blk in self.down_blocks:\n",
    "            x = blk(x, t_emb)\n",
    "            role_feats.append(x)\n",
    "        art_feats = self.cond_enc(artifact)\n",
    "        # bottleneck\n",
    "        d = self.mid(x, t_emb)\n",
    "        for i, up in enumerate(self.up_blocks):\n",
    "            d = up(d, t_emb)\n",
    "        d = self.skip_fus[-1](d, role_feats[0], art_feats[0])\n",
    "        return self.out_head(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51249464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "def train_epoch(model, loader, opt, sc, epoch):\n",
    "    model.train()\n",
    "    for art, role in tqdm(loader, desc=f\"Epoch {epoch}\"):\n",
    "        art, role = art.to(device), role.to(device)\n",
    "        B = role.size(0)\n",
    "        t = torch.randint(0, sc.N, (B,), device=device).long()\n",
    "        β, αbar = sc.get_params(t.cpu())\n",
    "        β, αbar = β.to(device), αbar.to(device)\n",
    "        noise = torch.randn_like(role)\n",
    "        noisy = (αbar.sqrt()[...,None,None,None]*role +\n",
    "                 (1-αbar).sqrt()[...,None,None,None]*noise)\n",
    "        ε_pred = model.forward(noisy, art, t)\n",
    "        loss = F.mse_loss(ε_pred, noise)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "061dd8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/50 [00:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "XPU out of memory. Tried to allocate 1728.08 GiB. GPU 0 has a total capacity of 11.60 GiB. Of the allocated memory 18.63 GiB is allocated by PyTorch, and 773.27 MiB is reserved by PyTorch but unallocated. Please use `empty_cache` to release all unoccupied cached memory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m loader\u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, opt, sc, epoch)\u001b[0m\n\u001b[0;32m     10\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(role)\n\u001b[0;32m     11\u001b[0m noisy \u001b[38;5;241m=\u001b[39m (αbar\u001b[38;5;241m.\u001b[39msqrt()[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39mrole \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     12\u001b[0m          (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mαbar)\u001b[38;5;241m.\u001b[39msqrt()[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39mnoise)\n\u001b[1;32m---> 13\u001b[0m ε_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(ε_pred, noise)\n\u001b[0;32m     15\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(); loss\u001b[38;5;241m.\u001b[39mbackward(); opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[12], line 38\u001b[0m, in \u001b[0;36mUNetDiff.forward\u001b[1;34m(self, role_noisy, artifact, t)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, up \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks):\n\u001b[0;32m     37\u001b[0m     d \u001b[38;5;241m=\u001b[39m up(d, t_emb)\n\u001b[1;32m---> 38\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_fus\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole_feats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mart_feats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head(d)\n",
      "File \u001b[1;32mc:\\Users\\TabYe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TabYe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m, in \u001b[0;36mSkipAttnFusion.forward\u001b[1;34m(self, dec, role, art)\u001b[0m\n\u001b[0;32m     15\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mreshape(B,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads,C\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mreshape(B,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads,C\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m     18\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m out  \u001b[38;5;241m=\u001b[39m (v \u001b[38;5;241m@\u001b[39m attn\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(B,C,H,W)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: XPU out of memory. Tried to allocate 1728.08 GiB. GPU 0 has a total capacity of 11.60 GiB. Of the allocated memory 18.63 GiB is allocated by PyTorch, and 773.27 MiB is reserved by PyTorch but unallocated. Please use `empty_cache` to release all unoccupied cached memory."
     ]
    }
   ],
   "source": [
    "# %% \n",
    "model = UNetDiff().to(device)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "loader= DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_epoch(model, loader, opt, schedule, epoch)\n",
    "    torch.save(model.state_dict(), f\"ckpt_epoch{epoch}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa6f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
